from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

print("Spark version", spark.version)

data_path = "./Data"
filepath = data_path + "/location_temp.csv"

df1 = spark.read.format("csv").option("header", "true").load(filepath)

df1.head(10)

# more nicely formatted version
df1.show(10)

# show the number of observations
df1.count()

# let's load another file
filepath_no_header = data_path + "/utilization.csv"

# Specify no header, set the option to infer the schema
df2 = spark.read.format("csv").option("header", "false").option("inferSchema", "true").load(filepath_no_header)
df2.show()
df.count()

# Spark autogenerated some column names
df2.show(10)

# Rename columns
df2 = df2.withColumnRenamed("_c0", "event_datetime") \
    .withColumnRenamed("_c1", "server_id") \
    .withColumnRenamed("_c2", "cpu_utilization") \
    .withColumnRenamed("_c3", "free_memory") \
    .withColumnRenamed("_c4", "session_count")

df2.show(10)

# df2.groupBy("server_id").count().show()

